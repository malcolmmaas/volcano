{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd638641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# using test env for this\n",
    "\n",
    "## Load packages\n",
    "from __future__ import absolute_import, division, print_function\n",
    "#..\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LogNorm\n",
    "from netCDF4 import num2date,date2num,Dataset\n",
    "import matplotlib._pylab_helpers\n",
    "import os,subprocess,time,copy\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "# import corner\n",
    "import pymc\n",
    "import pandas as pd\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import IPython\n",
    "\n",
    "# Malcolm's imports\n",
    "import netCDF4\n",
    "from multiprocessing.dummy import Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27eec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "# Auxilary functions for nn\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "#         if epoch % 100 == 0: print('')\n",
    "#         print('.', end='')\n",
    "        print(epoch, end='\\r')\n",
    "        \n",
    "        \n",
    "# Monitoring the progress of the model\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "    if 'val_mean_squared_error' in hist:\n",
    "        plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "               label = 'Val Error')\n",
    "    plt.plot(hist['epoch'],0.1+0.*hist['epoch'],'k:')\n",
    "    plt.plot(hist['epoch'],0.2+0.*hist['epoch'],'k:')\n",
    "    plt.plot(hist['epoch'],0.3+0.*hist['epoch'],'k:')\n",
    "    plt.plot(hist['epoch'],0.4+0.*hist['epoch'],'k:')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83a27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================  Monthly timeseries controls and constants  ==========================\n",
    "# After restarting the kernel, run this cell. It should give everything needed\n",
    "\n",
    "df = pd.read_csv('parameters.txt', delim_whitespace=True)#.drop(['VLHC126kt'])\n",
    "\n",
    "def julian_to_month(julian_day):\n",
    "    months = [\n",
    "        (1, 31), (2, 28), (3, 31), (4, 30), (5, 31), (6, 30),\n",
    "        (7, 31), (8, 31), (9, 30), (10, 31), (11, 30), (12, 31)\n",
    "    ]\n",
    "    for month, days in months:\n",
    "        if julian_day <= days:\n",
    "            return month\n",
    "        julian_day -= days\n",
    "        \n",
    "df['month'] = list(map(lambda x: julian_to_month(x), df['julday']))\n",
    "\n",
    "control = netCDF4.Dataset('timeseries/VLHC000kt/MON.taijVLHC000kt.nc')\n",
    "control_ant = netCDF4.Dataset('timeseries/VLHC000kt/MON.Antarctica.taijVLHC000kt.nc')\n",
    "\n",
    "earth_area = control['axyp'][0]\n",
    "ant_area = control_ant['axyp'][0]\n",
    "s_molar_mass = 32.065\n",
    "so2_mmass = 64.066\n",
    "dms_mmass = 62\n",
    "su_mmass = 96.06\n",
    "h2so4_mmass = 98.079\n",
    "s_per_year = 3.154e7\n",
    "s_per_month = 2.628e6\n",
    "# for converting a global flux (kg m^-2 s^-1) to g and tg\n",
    "flux_to_g = 1e3*s_per_month*earth_area\n",
    "flux_to_tg = s_per_month*earth_area/1e9\n",
    "\n",
    "su_dep_vars = []\n",
    "su_tm_vars = []\n",
    "so2_dep_vars = ['SO2_wet_dep', 'SO2_dry_dep']\n",
    "h2so4_dep_vars = ['H2SO4_wet_dep','H2SO4_dry_dep','H2SO4_gs_dep']\n",
    "\n",
    "# Must include all modes (ACC, AKK, OCC, etc)\n",
    "for v in control.variables.keys():\n",
    "    if 'SU_wet_dep' in v or 'SU_dry_dep' in v or 'SU_gs_dep' in v:\n",
    "        su_dep_vars.append(v)\n",
    "    if 'SU_Total_Mass' in v and not '_hemis' in v:\n",
    "        su_tm_vars.append(v)\n",
    "\n",
    "# Get monthly averages for all 10 years, return 10 years of averages\n",
    "def get_10yr_monthly_control(control_vals):\n",
    "    return np.array([np.sum(np.array(control_vals).reshape(10,12), axis=0)/10]*10).flatten()\n",
    "\n",
    "# start after 10th year because first 10 years are spin-up\n",
    "su_dep_control_10yr = get_10yr_monthly_control(sum([control[v][120:] for v in su_dep_vars]))\n",
    "so2_dep_control_10yr = get_10yr_monthly_control(sum([control[v][120:] for v in so2_dep_vars]))\n",
    "h2so4_dep_control_10yr = get_10yr_monthly_control(sum([control[v][120:] for v in h2so4_dep_vars]))\n",
    "su_tm_control_10yr = get_10yr_monthly_control(sum([control[v][120:] for v in su_tm_vars]))\n",
    "so2_tm_control_10yr = get_10yr_monthly_control(control['SO2_Total_Mass'][120:])\n",
    "h2so4_tm_control_10yr = get_10yr_monthly_control(control['H2SO4_Total_Mass'][120:])\n",
    "\n",
    "su_dep_control_10yr_ant = get_10yr_monthly_control(sum([control_ant[v][120:] for v in su_dep_vars]))\n",
    "so2_dep_control_10yr_ant = get_10yr_monthly_control(sum([control_ant[v][120:] for v in so2_dep_vars]))\n",
    "h2so4_dep_control_10yr_ant = get_10yr_monthly_control(sum([control_ant[v][120:] for v in h2so4_dep_vars]))\n",
    "su_tm_control_10yr_ant = get_10yr_monthly_control(sum([control_ant[v][120:] for v in su_tm_vars]))\n",
    "so2_tm_control_10yr_ant = get_10yr_monthly_control(control_ant['SO2_Total_Mass'][120:])\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_names = ['', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "\n",
    "# from https://www.unidata.ucar.edu/blogs/developer/en/entry/accessing_netcdf_data_by_coordinates\n",
    "def naive_fast(latvar,lonvar,lat0,lon0):\n",
    "    latvals = np.array([latvar[:]]*144)\n",
    "    lonvals = np.array([lonvar[:]]*90).transpose()\n",
    "    dist_sq = (latvals-lat0)**2 + (lonvals-lon0)**2\n",
    "    minindex_flattened = dist_sq.argmin()\n",
    "    iy_min,ix_min = np.unravel_index(minindex_flattened, latvals.shape)\n",
    "    return iy_min,ix_min\n",
    "\n",
    "def find_tropopause(row):\n",
    "    monthname = month_names[int(row['month'])]\n",
    "    test = netCDF4.Dataset('control_means_2d/' + monthname + '0011-0020.aijlVLHC000kt.nc')\n",
    "    \n",
    "    # find gridbox\n",
    "    lon_idx, lat_idx = naive_fast(test['lat'], test['lon'], row['latitude'], row['longitude'])\n",
    "    \n",
    "    tempL = test['TempL'][:,lat_idx,lon_idx]\n",
    "    z = test['z'][:,lat_idx,lon_idx]\n",
    "    \n",
    "    test = netCDF4.Dataset('control_means_2d/' + monthname + '0011-0020.aijVLHC000kt.nc')\n",
    "    \n",
    "    ttrop = test['ttrop'][lat_idx,lon_idx]\n",
    "    ltrop = np.argmin(np.abs(np.array(tempL)-ttrop))\n",
    "    \n",
    "    return z[ltrop]\n",
    "\n",
    "def get_ptrop(row):\n",
    "    monthname = month_names[int(row['month'])]\n",
    "    test = netCDF4.Dataset('control_means_2d/' + monthname + '0011-0020.aijVLHC000kt.nc')\n",
    "    \n",
    "    # find gridbox\n",
    "    lon_idx, lat_idx = naive_fast(test['lat'], test['lon'], row['latitude'], row['longitude'])\n",
    "    \n",
    "    return test['ptrop'][lat_idx,lon_idx]\n",
    "\n",
    "def get_stratospheric_so2(row):\n",
    "    if row['plumetop'] < row['tropopause']:\n",
    "        return 0\n",
    "    elif row['plumebottom'] > row['tropopause']:\n",
    "        return row['SO2']\n",
    "    else:\n",
    "        return row['SO2']*(row['plumetop']-row['tropopause'])/row['plumethickness']\n",
    "\n",
    "def get_stratospheric_so2_pct(row):\n",
    "    if row['plumetop'] < row['tropopause']:\n",
    "        return 0\n",
    "    elif row['plumebottom'] > row['tropopause']:\n",
    "        return 1\n",
    "    else:\n",
    "        return (row['plumetop']-row['tropopause'])/row['plumethickness']\n",
    "    \n",
    "df['tropopause'] = df.apply(lambda row: find_tropopause(row), axis=1)\n",
    "df['ptrop'] = df.apply(lambda row: get_ptrop(row), axis=1)\n",
    "df['stratospheric_so2'] = df.apply(lambda row: get_stratospheric_so2(row), axis=1)\n",
    "df['tropospheric_so2'] = df['SO2'] - df['stratospheric_so2']\n",
    "df['stratospheric_so2_pct'] = df.apply(lambda row: get_stratospheric_so2_pct(row), axis=1)\n",
    "# df_south = df[df['latitude'] <= -30]\n",
    "# df_tropic = df[(df['latitude'] > -30) & (df['latitude'] < 30)]\n",
    "# df_north = df[df['latitude'] >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f8b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLHC140kt\r"
     ]
    }
   ],
   "source": [
    "su_deps = []\n",
    "for run in df.index:\n",
    "    print(run, end='\\r')\n",
    "    test = netCDF4.Dataset('timeseries/'+run+'/MON.Antarctica.taij'+run+'.nc')\n",
    "    test_su_dep = sum([test[v][:] for v in su_dep_vars])-su_dep_control_10yr_ant\n",
    "    test_su_dep = test_su_dep*flux_to_tg/su_mmass\n",
    "    m = int(df.loc[run]['month'])-1\n",
    "    # shift so it's months since eruption, get first 4 years\n",
    "    test_4yr_su_dep = np.sum(test_su_dep[m:len(test_su_dep)-12+m].reshape(9,12)[:4], axis=1)\n",
    "    su_deps.append(test_4yr_su_dep)\n",
    "\n",
    "params = ['longitude', 'latitude', 'julday', 'plumebottom', 'plumetop', 'H2O', 'stratospheric_so2', 'tropospheric_so2']\n",
    "X = df[params]\n",
    "y = np.array(su_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26be05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X.copy()\n",
    "X_scaled['latitude'] = np.sin(X['latitude']*np.pi/180)\n",
    "\n",
    "bounds = {\n",
    "    'longitude': [-180,180],\n",
    "    'latitude': [-1,1],\n",
    "    'julday': [0,365],\n",
    "    'plumebottom': [2000,35000],\n",
    "    'plumetop': [3000,45000],\n",
    "    'H2O': [0,2000],\n",
    "    'stratospheric_so2': [2,100],\n",
    "    'tropospheric_so2': [2,100],\n",
    "}\n",
    "\n",
    "bounds_df = pd.DataFrame(bounds)\n",
    "\n",
    "mmscaler = MinMaxScaler().fit(bounds_df)\n",
    "X_scaled = mmscaler.transform(X_scaled)\n",
    "# sscaler = StandardScaler()\n",
    "# X_scaled = sscaler.fit_transform(X_scaled)\n",
    "# X_scaled = X\n",
    "\n",
    "# sscaler = StandardScaler()\n",
    "# y_scaled = sscaler.fit_transform(y)\n",
    "mmscaler = MinMaxScaler()\n",
    "y_scaled = mmscaler.fit_transform(y)\n",
    "# y_scaled = y\n",
    "\n",
    "# mmscaler.inverse_transform(X_scaled)\n",
    "\n",
    "split_index = 110\n",
    "rf_model = RandomForestRegressor(random_state=6)\n",
    "rf_model.fit(X_scaled[:split_index], y_scaled[:split_index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec2dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15435a178b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a nn model for the training\n",
    "nn_model = keras.Sequential([layers.Dense(32, activation=tf.nn.relu, input_shape=[8]),\n",
    "                          layers.Dense(32, kernel_initializer='normal',activation='relu'),\n",
    "                          layers.Dense(32, kernel_initializer='normal',activation='relu'),\n",
    "#                           layers.Dense(32, kernel_initializer='normal',activation='relu'),\n",
    "#                               layers.Dense(32, kernel_initializer='normal',activation='relu'),\n",
    "#                               layers.Dense(32, kernel_initializer='normal',activation='relu'),\n",
    "                          layers.Dense(4)])\n",
    "\n",
    "nn_model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "EPOCHS = 1000\n",
    "nn_model.fit(X_scaled[:split_index], y_scaled[:split_index], epochs=EPOCHS,\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c6ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "Train NN: 1.230877e-05\n",
      "Test NN: 0.08977283\n",
      "Train RF: 0.006355798845615673\n",
      "Test RF: 0.11158356385151222\n"
     ]
    }
   ],
   "source": [
    "test_nn_score = mean_squared_error(y_scaled[split_index:], nn_model.predict(X_scaled[split_index:]))\n",
    "test_rf_score = mean_squared_error(y_scaled[split_index:], rf_model.predict(X_scaled[split_index:]))\n",
    "train_nn_score = mean_squared_error(y_scaled[:split_index], nn_model.predict(X_scaled[:split_index]))\n",
    "train_rf_score = mean_squared_error(y_scaled[:split_index], rf_model.predict(X_scaled[:split_index]))\n",
    "print('Train NN: '+str(train_nn_score))\n",
    "print('Test NN: '+str(test_nn_score))\n",
    "print('Train RF: '+str(train_rf_score))\n",
    "print('Test RF: '+str(test_rf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5931333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the loglikelihood function\n",
    "### Need to be double checked\n",
    "### Users need to define it on their own\n",
    "\n",
    "def loglike_grad(para, obs, sigma):\n",
    "    n_para = len(para)\n",
    "    para = list([para])\n",
    "    \n",
    "    para = tf.constant(para)\n",
    "        ## To be changed in the future, redundant\n",
    "    sigma_sq = sigma**2\n",
    "    \n",
    "    dloglike_dx = np.empty(n_para)\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(para)\n",
    "#         log_like = tf.math.reduce_sum(tf.math.divide((obs - tf.constant(rf_model.predict(para)))**2,(sigma_sq * 2)) * (-1))\n",
    "        log_like = tf.math.reduce_sum(tf.math.divide((obs - nn_model(para))**2,(sigma_sq * 2)) * (-1))\n",
    "    \n",
    "    log_like = tf.cast(log_like, tf.float64)\n",
    "    \n",
    "#     dloglike_dx = t.gradient(log_like, para)\n",
    "#     dloglike_dx = tf.cast(dloglike_dx, tf.float64)\n",
    "    \n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c04c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Op that connects pymc with the likelihood function defined above \n",
    "class LogLike(pt.Op):\n",
    "    itypes = [pt.dvector]  # expects a vector of parameter values when called\n",
    "    otypes = [pt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, loglike, obs, sigma):\n",
    "        # add inputs as class attributes\n",
    "        self.likelihood = loglike\n",
    "        self.obs = obs\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        (para,) = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(para, self.obs, self.sigma)\n",
    "\n",
    "        outputs[0][0] = np.array(logl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c7c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01240636  0.47441974  0.2776443   0.06879375]]\n",
      "in pm.model stuff\n",
      "[longitude, latitude, julday, plumebottom, plumetop, H2O, stratospheric_so2, tropospheric_so2]\n",
      "MakeVector{dtype='float64'}.0\n",
      "calling pm.sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (2 chains in 1 job)\n",
      "CompoundStep\n",
      ">Slice: [longitude]\n",
      ">Slice: [latitude]\n",
      ">Slice: [julday]\n",
      ">Slice: [plumebottom]\n",
      ">Slice: [plumetop]\n",
      ">Slice: [H2O]\n",
      ">Slice: [stratospheric_so2]\n",
      ">Slice: [tropospheric_so2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1200 00:00<00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define obs and uncertainties\n",
    "case = 10\n",
    "obs = np.array([y_scaled[case]])\n",
    "print(obs)\n",
    "sigma = np.array([0.05,0.05,0.05,0.05])   \n",
    "## Initiate the loglikelihood function\n",
    "logl = LogLike(loglike_grad, obs, sigma)\n",
    "\n",
    "## Start the training\n",
    "with pm.Model() as opmodel:\n",
    "    print('in pm.model stuff')\n",
    "    # set priors\n",
    "#     para = [pm.Uniform(p, lower=min(X_scaled[:,i]), upper=max(X_scaled[:,i])) for i,p in enumerate(X.columns)]\n",
    "    para = [pm.Uniform(p, lower=min(bounds[p]), upper=max(bounds[p])) for i,p in enumerate(X.columns)]\n",
    "    print(para)\n",
    "    para = pt.as_tensor_variable(para)\n",
    "    print(para)\n",
    "\n",
    "        # Use what Kostas used in LHC---\n",
    "        # Correct latitude prior---?\n",
    "\n",
    "        # min-max scaling for inputs---\n",
    "        # standardization for outputs---\n",
    "\n",
    "        # run chains for longer---\n",
    "        # corner plots\n",
    "    \n",
    "    pm.Potential(\"likelihood\", logl(para))\n",
    "    print('calling pm.sample')\n",
    "    idata_mh = pm.sample(1000, tune=200, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner plots:\n",
    "# https://corner.readthedocs.io/en/latest/\n",
    "\n",
    "# https://samreay.github.io/ChainConsumer/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683c2c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Check the results\n",
    "lines = [(p, {}, X_scaled[case][i]) for i,p in enumerate(X.columns)]\n",
    "az.plot_trace(idata_mh, lines=lines);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a14b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
